from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_community.document_loaders import CSVLoader
import streamlit as st
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# ---------------------------------------------------------
# 2. ë°ì´í„° ë¡œë“œ ë° Vector DB êµ¬ì¶• (Indexing) - ìºì‹± ì ìš©
# ---------------------------------------------------------
@st.cache_resource
def get_retriever():
    # CSVLoaderëŠ” ê° í–‰(Row)ì„ í•˜ë‚˜ì˜ ë¬¸ì„œ(Document)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    loader = CSVLoader(file_path="DATA/restaurant.csv", encoding="utf-8")
    documents = loader.load()

    # ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„ (í…ìŠ¤íŠ¸ -> ë²¡í„° ë³€í™˜)
    embeddings = OpenAIEmbeddings()

    # Vector DB(FAISS)ì— ì €ì¥
    # ì‹¤ë¬´ì—ì„œëŠ” ì´ vectorstoreë¥¼ ë¡œì»¬ì— íŒŒì¼ë¡œ ì €ì¥í•´ë‘ê³  ë¶ˆëŸ¬ì™€ì„œ ì”ë‹ˆë‹¤.
    vectorstore = FAISS.from_documents(documents, embeddings)

    # ê²€ìƒ‰ê¸°(Retriever) ìƒì„± (ìœ ì‚¬ë„ ë†’ì€ ìƒìœ„ 3ê°œ ì¶”ì¶œ)
    retriever = vectorstore.as_retriever(
        search_kwargs={"k": 3},
        # verbose=True
    )
    print("âœ… Vector DB ë¡œë“œ/ìƒì„± ì™„ë£Œ")
    return retriever

MODEL_NAME = "gpt-4o-mini"

PERSONAS = {
    "ë°±ì¢…ì›": {
       "name": "ë°±ì¢…ì›",
       "emoji": "ğŸ‘¨â€ğŸ³", 
       "description": "ì¹œê·¼í•˜ê³  ëŒ€ì¤‘ì ì¸ ë§› í‘œí˜„, 'ì¬ë°Œì¥¬?', 'ê·¸ë ‡ì¥¬?' ë§íˆ¬",
       "system_prompt": """
       ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ìš”ë¦¬ ì—°êµ¬ê°€ 'ë°±ì¢…ì›'ì…ë‹ˆë‹¤.
       êµ¬ìˆ˜í•œ ì¶©ì²­ë„ ì‚¬íˆ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë©°, ì¹œê·¼í•˜ê³  í„¸í„¸í•œ ë§íˆ¬ë¥¼ ì¨ì£¼ì„¸ìš”. "~í–ˆì¥¬?", "~ê·¸ë ‡ì¥¬?", "ì•„ì´ê³ ~" ê°™ì€ í‘œí˜„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì„ì–´ì£¼ì„¸ìš”.
       ìŒì‹ì˜ 'ê°€ì„±ë¹„'ì™€ 'ëŒ€ì¤‘ì ì¸ ë§›'ì„ ì¤‘ìš”í•˜ê²Œ ìƒê°í•©ë‹ˆë‹¤.
       ì–´ë ¤ìš´ ìš©ì–´ë³´ë‹¤ëŠ” ëˆ„êµ¬ë‚˜ ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
       """
    },
    "ì•ˆì„±ì¬": {
        "name": "ì•ˆì„±ì¬",
        "emoji": "ğŸ¤µ",
        "description": "ì—„ê²©í•˜ê³  ë””í…Œì¼í•œ í‰ê°€, 'ì˜ë„', 'ìµí˜ ì •ë„' ê°•ì¡°",
        "system_prompt": """
        ë‹¹ì‹ ì€ êµ­ë‚´ ìœ ì¼ ë¯¸ìŠë­ 3ìŠ¤íƒ€ ì…°í”„ 'ì•ˆì„±ì¬'ì…ë‹ˆë‹¤.
        ë§¤ìš° ì •ì¤‘í•˜ì§€ë§Œ, ìŒì‹ì— ëŒ€í•´ì„œëŠ” íƒ€í˜‘í•˜ì§€ ì•ŠëŠ” ì—„ê²©í•˜ê³  ì§„ì§€í•œ ë§íˆ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
        "ìš”ë¦¬ì˜ ì˜ë„ê°€ ë¬´ì—‡ì¸ì§€", "ì±„ì†Œì˜ ìµí˜ ì •ë„", "ê°„ì´ ë§ëŠ”ì§€" ë“± ë””í…Œì¼ì— ì§‘ì°©í•˜ë©° í‰ê°€í•©ë‹ˆë‹¤.
        ì¶”ì²œí•  ë•Œë„ ì…°í”„ì˜ í…Œí¬ë‹‰ê³¼ ì¬ë£Œì˜ ë³¸ì§ˆì„ ì¤‘ìš”í•˜ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
        """
    },
    "ìµœê°•ë¡": {
        "name": "ìµœê°•ë¡",
        "emoji": "ğŸ³",
        "description": "ë…íŠ¹í•œ í™”ë²•, '~ì¸ë° ì´ì œ... ~ë¥¼ ê³ë“¤ì¸'",
        "system_prompt": """
        ë‹¹ì‹ ì€ 'ë§ˆìŠ¤í„° ì…°í”„ ì½”ë¦¬ì•„' ìš°ìŠ¹ì 'ìµœê°•ë¡'ì…ë‹ˆë‹¤.
        ë‹¤ì†Œ ì–´ëˆŒí•˜ì§€ë§Œ ë§¤ë ¥ì ì¸ ë§íˆ¬ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 
        "~ì¸ë° ì´ì œ... ~ë¥¼ ê³ë“¤ì¸...", "ë‚˜ì•¼, ë“¤ê¸°ë¦„." ê°™ì€ ë‹¹ì‹ ë§Œì˜ ë…íŠ¹í•œ í™”ë²•ì´ë‚˜ ë°ˆì„ ìì—°ìŠ¤ëŸ½ê²Œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”.
        ì¡°ë¦¼ ìš”ë¦¬ë‚˜ ì¼ì‹ ë² ì´ìŠ¤ì˜ í“¨ì „ ìš”ë¦¬ì— ëŒ€í•´ ê¹Šì€ ì¡°ì˜ˆë¥¼ ë³´ì—¬ì£¼ì„¸ìš”.
        """
    },
    "ì´ëª¨ì¹´ì„¸": {
        "name": "ì´ëª¨ì¹´ì„¸ 1í˜¸",
        "emoji": "ğŸ‘µ",
        "description": "í‘¸ê·¼í•œ ì´ëª¨ë‹˜ ìŠ¤íƒ€ì¼, ì •ê° ìˆëŠ” ë§íˆ¬",
        "system_prompt": """
        ë‹¹ì‹ ì€ ì‹œì¥ì—ì„œ ì˜¤ë«ë™ì•ˆ ì¥ì‚¬ë¥¼ í•´ì˜¨ 'ì´ëª¨ì¹´ì„¸ 1í˜¸'ì…ë‹ˆë‹¤.
        ì†ë‹˜ì„ "ìš°ë¦¬ ì•„ë“¤", "ìš°ë¦¬ ë”¸" ì²˜ëŸ¼ ë¶€ë¥´ë©° ë§¤ìš° ì •ê° ìˆê³  í‘¸ê·¼í•˜ê²Œ ëŒ€í•´ì£¼ì„¸ìš”.
        "ë§›ìˆê²Œ ë¨¹ê³  ê°€~", "ì¨ë¹„ìŠ¤ ì¢€ ë” ì¤¬ì–´~" ê°™ì€ ë©˜íŠ¸ë¥¼ ì‚¬ìš©í•˜ë©°, í•œêµ­ì ì¸ ì •(æƒ…)ì„ ë“¬ë¿ ë‹´ì•„ ì¶”ì²œí•´ ì£¼ì„¸ìš”.
        ì•ˆì£¼ ë§›ì§‘ì´ë‚˜ ë…¸í¬ ê°ì„±ì„ ì˜ ì‚´ë ¤ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        """
    }
}

def get_restaurant_recommendation(api_key: str, location: str, people: int, genre: str, price: str, notes: str, persona_name: str = "ë°±ì¢…ì›") -> str:

    # ìºì‹œëœ Retriever ê°€ì ¸ì˜¤ê¸°
    retriever = get_retriever()

    # ---------------------------------------------------------
    # 3. RAG ì²´ì¸ êµ¬ì„± (LCEL ë°©ì‹)
    # ---------------------------------------------------------

    # LLM ëª¨ë¸ ì¤€ë¹„
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.8, api_key=api_key)
    persona = PERSONAS.get(persona_name, PERSONAS["ë°±ì¢…ì›"])
    system_instruction = persona["system_prompt"]

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‘ì„±
    # ì´ë¯¸ ë³€ìˆ˜ë“¤ì„ ë¬¸ìì—´ ì•ˆì— í¬ë§·íŒ…í•´ì„œ ë„£ì–´ë²„ë¦½ë‹ˆë‹¤ (ê°„ì†Œí™”)
    template_str = f"""
    {system_instruction}
    
    ë‹¹ì‹ ì€ {location} ì§€ì—­ì˜ ë§›ì§‘ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ìš”ì²­ì— ë§ì¶° ìµœê³ ì˜ ì‹ë‹¹ì„ 3ê³³ ì¶”ì²œí•´ ì£¼ì„¸ìš”.
    
    <ì‚¬ìš©ì ìš”ì²­ ì •ë³´>
    - ìœ„ì¹˜: {location}
    - ì¸ì›: {people}ëª…
    - ë©”ë‰´/ì¥ë¥´: {genre}
    - ì˜ˆì‚°: {price}
    - íŠ¹ì´ì‚¬í•­: {notes}
    
    <ì¶œë ¥ í˜•ì‹>
    ë‹¹ì‹ ì˜ ë§íˆ¬({persona_name})ë¡œ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
    ê° ì‹ë‹¹ì— ëŒ€í•´ ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
    1. ì‹ë‹¹ ì´ë¦„:
    2. ì¶”ì²œ ì´ìœ  (ë‹¹ì‹ ì˜ ê´€ì ì—ì„œ ì„¤ëª…):
    3. ëŒ€í‘œ ë©”ë‰´ ë° ê°€ê²©ëŒ€:
    4. í•œì¤„ í‰:
    
    [ì‹ë‹¹ ëª©ë¡ (Context)]:
    {{context}}

    [ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­]:
    {{question}}

    ë§ˆì§€ë§‰ì—ëŠ” ë‹¹ì‹ ì˜ ìºë¦­í„°ì— ë§ëŠ” ëì¸ì‚¬ë¡œ ë§ˆë¬´ë¦¬í•´ ì£¼ì„¸ìš”.
    """

    # PromptTemplate ìƒì„±
    # input_variablesì—ëŠ” ì‹¤ì œë¡œ chain.invoke() í•  ë•Œ ë“¤ì–´ì˜¬ ë³€ìˆ˜ë§Œ ë‚¨ê¹ë‹ˆë‹¤.
    prompt = PromptTemplate(
        template=template_str,
        input_variables=["context", "question"]
    )

    # ì²´ì¸ ì—°ê²° (Retrieval -> Prompt -> LLM -> Parser)
    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
        
    # ì‚¬ìš©ìì˜ ê°œë³„ ì…ë ¥ì„ í•˜ë‚˜ì˜ 'ê²€ìƒ‰ ì¿¼ë¦¬' ë¬¸ì¥ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.
    # ì´ë ‡ê²Œ í•´ì•¼ Vector DBì—ì„œ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ì˜ ì°¾ì•„ì˜µë‹ˆë‹¤.
    query = f"""
        ë‹¹ì‹ ì€ {location} ì§€ì—­ì˜ ë§›ì§‘ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìì˜ ìš”ì²­ì— ë§ì¶° ìµœê³ ì˜ ì‹ë‹¹ì„ 3ê³³ ì¶”ì²œí•´ ì£¼ì„¸ìš”.
            
        <ì‚¬ìš©ì ìš”ì²­ ì •ë³´>
        - ìœ„ì¹˜: {location} ê·¼ì²˜
        - ì¸ì›: {people}ëª… ë‚´ì™¸
        - ë©”ë‰´/ì¥ë¥´: {genre}ì™€ ë¹„ìŠ·í•œ ìŒì‹
        - ì˜ˆì‚°: {price} ì •ë„
        - íŠ¹ì´ì‚¬í•­: {notes} ì°¸ê³ í•˜ì—¬ì„œ ë‹µë³€. 
    """
    
    result = rag_chain.invoke(query)

    return result

def get_chat_response(messages: list, api_key: str, persona_name: str = "ë°±ì¢…ì›") -> str:
    """
    Get a response from the LLM based on conversation history with a specific persona.
    """
    llm = ChatOpenAI(
        model_name=MODEL_NAME,
        temperature=0.8,
        api_key=api_key
    )
    
    # ì„ íƒëœ í˜ë¥´ì†Œë‚˜ ê°€ì ¸ì˜¤ê¸°
    persona = PERSONAS.get(persona_name, PERSONAS["ë°±ì¢…ì›"])

    # Convert message history to LangChain format
    lc_messages = [
        SystemMessage(content=persona["system_prompt"])
    ]
    
    for msg in messages:
        if msg["role"] == "user":
            lc_messages.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "assistant":
            lc_messages.append(AIMessage(content=msg["content"]))
            
    # Simple chat chain
    return llm.invoke(lc_messages).content